{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edd262e-d4fe-46b0-8e1e-f116d2179b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1934a6-f9c8-471d-b239-f5a999e304a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415c4d5a-7100-48b6-b1a3-5b672e505873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('spoken_train-v1.1.json', 'rb') as f:\n",
    "  squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e260f80a-c644-4c3a-a77f-42b45f1dd56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'paragraphs'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['data'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b760a4f-26bd-4188-b176-9f20bccaa9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n",
      "Beyoncé\n",
      "Montana\n",
      "Genocide\n",
      "Antibiotics\n",
      "Frédéric_Chopin\n",
      "Sino-Tibetan_relations_during_the_Ming_dynasty\n",
      "IPod\n",
      "The_Legend_of_Zelda:_Twilight_Princess\n",
      "Spectre_(2015_film)\n",
      "2008_Sichuan_earthquake\n",
      "New_York_City\n",
      "To_Kill_a_Mockingbird\n",
      "Solar_energy\n",
      "Tajikistan\n",
      "Anthropology\n",
      "Portugal\n",
      "Kanye_West\n",
      "Buddhism\n",
      "American_Idol\n",
      "Dog\n",
      "2008_Summer_Olympics_torch_relay\n",
      "Alfred_North_Whitehead\n",
      "Financial_crisis_of_2007%E2%80%9308\n",
      "Saint_Barth%C3%A9lemy\n",
      "Genome\n",
      "Comprehensive_school\n",
      "Republic_of_the_Congo\n",
      "Prime_minister\n",
      "Institute_of_technology\n",
      "Wayback_Machine\n",
      "Dutch_Republic\n",
      "Symbiosis\n",
      "Canadian_Armed_Forces\n",
      "Cardinal_(Catholicism)\n",
      "Iranian_languages\n",
      "Lighting\n",
      "Separation_of_powers_under_the_United_States_Constitution\n",
      "Architecture\n",
      "Human_Development_Index\n",
      "Southern_Europe\n",
      "BBC_Television\n",
      "Arnold_Schwarzenegger\n",
      "Plymouth\n",
      "Heresy\n",
      "Warsaw_Pact\n",
      "Materialism\n",
      "Space_Race\n",
      "Pub\n",
      "Christian\n",
      "Sony_Music_Entertainment\n",
      "Oklahoma_City\n",
      "Hunter-gatherer\n",
      "United_Nations_Population_Fund\n",
      "Russian_Soviet_Federative_Socialist_Republic\n",
      "Universal_Studios\n",
      "Alexander_Graham_Bell\n",
      "Internet_service_provider\n",
      "Comics\n",
      "Saint_Helena\n",
      "Aspirated_consonant\n",
      "Hydrogen\n",
      "Web_browser\n",
      "Boston\n",
      "BeiDou_Navigation_Satellite_System\n",
      "Canon_law\n",
      "Communications_in_Somalia\n",
      "Catalan_language\n",
      "Estonian_language\n",
      "Paper\n",
      "Arena_Football_League\n",
      "Adult_contemporary_music\n",
      "Matter\n",
      "Westminster_Abbey\n",
      "Nanjing\n",
      "Bern\n",
      "Daylight_saving_time\n",
      "Royal_Institute_of_British_Architects\n",
      "National_Archives_and_Records_Administration\n",
      "Tristan_da_Cunha\n",
      "University_of_Kansas\n",
      "Political_corruption\n",
      "Dialect\n",
      "Classical_music\n",
      "Slavs\n",
      "Southampton\n",
      "Treaty\n",
      "Josip_Broz_Tito\n",
      "Marshall_Islands\n",
      "Szlachta\n",
      "Virgil\n",
      "Alps\n",
      "Gene\n",
      "Guinea-Bissau\n",
      "List_of_numbered_streets_in_Manhattan\n",
      "Brain\n",
      "Near_East\n",
      "Zhejiang\n",
      "Ministry_of_Defence_(United_Kingdom)\n",
      "High-definition_television\n",
      "Wood\n",
      "Somalis\n",
      "Middle_Ages\n",
      "Phonology\n",
      "Computer\n",
      "Black_people\n",
      "The_Times\n",
      "New_Delhi\n",
      "Imamah_(Shia_doctrine)\n",
      "Bird_migration\n",
      "Atlantic_City,_New_Jersey\n",
      "Immunology\n",
      "MP3\n",
      "House_music\n",
      "Letter_case\n",
      "Chihuahua_(state)\n",
      "Pitch_(music)\n",
      "England_national_football_team\n",
      "Houston\n",
      "Copper\n",
      "Identity_(social_science)\n",
      "Himachal_Pradesh\n",
      "Communication\n",
      "Grape\n",
      "Computer_security\n",
      "Orthodox_Judaism\n",
      "Animal\n",
      "Beer\n",
      "Race_and_ethnicity_in_the_United_States_Census\n",
      "United_States_dollar\n",
      "Imperial_College_London\n",
      "Gymnastics\n",
      "Hanover\n",
      "Emotion\n",
      "FC_Barcelona\n",
      "Everton_F.C.\n",
      "Old_English\n",
      "Aircraft_carrier\n",
      "Federal_Aviation_Administration\n",
      "Lancashire\n",
      "Mesozoic\n",
      "Videoconferencing\n",
      "Gregorian_calendar\n",
      "Xbox_360\n",
      "Military_history_of_the_United_States\n",
      "Hard_rock\n",
      "Great_Plains\n",
      "Infrared\n",
      "Biodiversity\n",
      "ASCII\n",
      "Digestion\n",
      "Federal_Bureau_of_Investigation\n",
      "Adolescence\n",
      "Antarctica\n",
      "Mary_(mother_of_Jesus)\n",
      "Melbourne\n",
      "John,_King_of_England\n",
      "Macintosh\n",
      "Anti-aircraft_warfare\n",
      "Sanskrit\n",
      "Valencia\n",
      "General_Electric\n",
      "United_States_Army\n",
      "Franco-Prussian_War\n",
      "Eritrea\n",
      "Uranium\n",
      "Order_of_the_British_Empire\n",
      "Age_of_Enlightenment\n",
      "Circadian_rhythm\n",
      "Elizabeth_II\n",
      "Sexual_orientation\n",
      "Dell\n",
      "Capital_punishment_in_the_United_States\n",
      "Nintendo_Entertainment_System\n",
      "Ashkenazi_Jews\n",
      "Athanasius_of_Alexandria\n",
      "Seattle\n",
      "Memory\n",
      "Multiracial_American\n",
      "Pharmaceutical_industry\n",
      "Umayyad_Caliphate\n",
      "Asphalt\n",
      "Queen_Victoria\n",
      "Freemasonry\n",
      "Israel\n",
      "Hellenistic_period\n",
      "Napoleon\n",
      "Bill_%26_Melinda_Gates_Foundation\n",
      "Northwestern_University\n",
      "Hokkien\n",
      "Montevideo\n",
      "Poultry\n",
      "Arsenal_F.C.\n",
      "Dutch_language\n",
      "Buckingham_Palace\n",
      "Incandescent_light_bulb\n",
      "Clothing\n",
      "Chicago_Cubs\n",
      "States_of_Germany\n",
      "Korean_War\n",
      "Royal_Dutch_Shell\n",
      "Copyright_infringement\n",
      "Greece\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "gr = -1\n",
    "for idx in range(len(squad['data'])):\n",
    "    print(squad['data'][idx]['title'])\n",
    "    if squad['data'][idx]['title'] == 'Greece':\n",
    "        gr = idx\n",
    "        print(gr)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fefc55d-deee-4b59-8400-b186738ed9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'napoleon bonaparte nato mean l l j e n french nap led the napa tea or nepali on deep wanna parte the fifteenth of august seventeen sixty nine to the fifth of may eighteen twenty one was a french military and political leader who rose to prominence during the french revolution and led several successful campaigns during the revolutionary war sir. as napoleon i he was emperor of the french from eighty know for intel eighteen fourteen and again in eighteen fifteen. napoleon dominated european in global affairs for more than a decade while leaving france against a series of coalitions in the napoleonic wars. he won most of these wars and the vast majority of his battles building a large empire that ruled over continental europe before its final collapse in eighteen fifteen. often considered one of the greatest commanders in history his wars in campaigns are studied at military schools worldwide. he also remains one of the most celebrated and controversial political figures in western history. in civil affairs napoleon had a major longterm impact by bringing liberal reforms to the territories that he conquered especially the low countries switzerland and large parts of modern italy and germany. he implemented fundamental liberal policies in france and throughout western europe no wine his lasting legal achievement the napoleonic code has been adopted in various forms by a quarter of the worlds legal systems from japan to put back.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['data'][186]['paragraphs'][0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ee7b2d-9ace-4816-b908-e10dcfc6e14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(path):  \n",
    "  # load the json file\n",
    "  with open(path, 'rb') as f:\n",
    "    squad = json.load(f)\n",
    "\n",
    "  contexts, questions, answers = [], [], []\n",
    "  contexts.extend(passage['context'] for group in squad['data'] for passage in group['paragraphs'] for qa in passage['qas'] for _ in qa['answers'])\n",
    "  questions.extend(qa['question'] for group in squad['data'] for passage in group['paragraphs'] for qa in passage['qas'] for _ in qa['answers'])\n",
    "  answers.extend(answer for group in squad['data'] for passage in group['paragraphs'] for qa in passage['qas'] for answer in qa['answers'])\n",
    "\n",
    "  return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dc780f-70ed-4280-a431-ff76fcae158c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_data('spoken_train-v1.1.json')\n",
    "valid_contexts, valid_questions, valid_answers = read_data('spoken_test-v1.1.json')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7267a6-43e6-45f6-8fd0-13232b7d5f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37111 questions\n",
      "What country borders south Estonia?\n",
      "{'answer_start': 262, 'text': 'latvia'}\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(train_questions)} questions')\n",
    "print(train_questions[-10000])\n",
    "print(train_answers[-10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ff99a4-3174-4ee8-a835-80b717f03e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "  for i in range(len(answers)):\n",
    "    gold_text = answers[i]['text']\n",
    "    start_idx = answers[i]['answer_start']\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "\n",
    "    # Check the exact match first, then adjust if necessary\n",
    "    if contexts[i][start_idx:end_idx] == gold_text:\n",
    "      answers[i]['answer_end'] = end_idx\n",
    "    else:\n",
    "      # Try shifting the start and end indices until the match is found\n",
    "      for shift in range(1, 3):\n",
    "        if contexts[i][start_idx - shift:end_idx - shift] == gold_text:\n",
    "          answers[i]['answer_start'] = start_idx - shift\n",
    "          answers[i]['answer_end'] = end_idx - shift\n",
    "          break  # Stop once the correct indices are found\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(valid_answers, valid_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320b83d5-6344-4bf5-95cf-baf4b6a961e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What country borders south Estonia?\n",
      "{'answer_start': 262, 'text': 'latvia', 'answer_end': 268}\n"
     ]
    }
   ],
   "source": [
    "print(train_questions[-10000])\n",
    "print(train_answers[-10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af883b42-7393-4aa7-93cb-f5c02408f867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "609\n",
      "261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch: 100%|██████████| 4639/4639 [04:48<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.5751643673269675      Train Loss: 1.2458413775193586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [02:48<00:00, 94.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5317225191231377\n",
      "epoch 0 : 1.6405079857492548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch: 100%|██████████| 4639/4639 [04:49<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7376185600377023      Train Loss: 0.6111324815893449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [02:49<00:00, 93.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5432685910882615\n",
      "epoch 1 : 1.414313758755241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch: 100%|██████████| 4639/4639 [04:49<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8213535090745793      Train Loss: 0.3527273701356917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [02:48<00:00, 94.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5364986629137747\n",
      "epoch 2 : 1.5088824798235623\n",
      "WER List: [1.6405079857492548, 1.414313758755241, 1.5088824798235623]\n",
      "F1 Scores: [0.5317225191231377, 0.5432685910882615, 0.5364986629137747]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "num_questions, num_posible, num_imposible = 0, 0, 0\n",
    "\n",
    "def get_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    contexts, questions, answers = [], [], []\n",
    "    num_q, num_pos, num_imp = 0, 0, 0\n",
    "\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                num_q += 1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context.lower())\n",
    "                    questions.append(qa['question'].lower())\n",
    "                    answers.append(answer)\n",
    "    return num_q, num_pos, num_imp, contexts, questions, answers\n",
    "\n",
    "num_q, num_pos, num_imp, train_contexts, train_questions, train_answers = get_data('spoken_train-v1.1.json')\n",
    "num_questions, num_posible, num_imposible = num_q, num_pos, num_imp\n",
    "num_q, num_pos, num_imp, valid_contexts, valid_questions, valid_answers = get_data('spoken_test-v1.1.json')\n",
    "\n",
    "def add_answer_end(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "add_answer_end(train_answers, train_contexts)\n",
    "add_answer_end(valid_answers, valid_contexts)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "doc_stride = 128\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "pad_on_right = tokenizerFast.padding_side == \"right\"\n",
    "\n",
    "train_contexts_trunc = []\n",
    "for i in range(len(train_contexts)):\n",
    "    if len(train_contexts[i]) > MAX_LENGTH:\n",
    "        answer_start = train_answers[i]['answer_start']\n",
    "        answer_end = train_answers[i]['answer_start'] + len(train_answers[i]['text'])\n",
    "        mid = (answer_start + answer_end) // 2\n",
    "        para_start = max(0, min(mid - MAX_LENGTH // 2, len(train_contexts[i]) - MAX_LENGTH))\n",
    "        para_end = para_start + MAX_LENGTH\n",
    "        train_contexts_trunc.append(train_contexts[i][para_start:para_end])\n",
    "        train_answers[i]['answer_start'] = int((MAX_LENGTH / 2) - len(train_answers[i]['text']) // 2)\n",
    "    else:\n",
    "        train_contexts_trunc.append(train_contexts[i])\n",
    "\n",
    "train_encodings_fast = tokenizerFast(\n",
    "    train_questions,\n",
    "    train_contexts_trunc,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    stride=doc_stride,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "valid_encodings_fast = tokenizerFast(\n",
    "    valid_questions,\n",
    "    valid_contexts,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    stride=doc_stride,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "def ret_Answer_start_and_end(encodings, answers, idx):\n",
    "    ret_start, ret_end = 0, 0\n",
    "    answer_encoding_fast = tokenizerFast(answers[idx]['text'], max_length=MAX_LENGTH, truncation=True, padding=True)\n",
    "    input_ids = encodings['input_ids'][idx]\n",
    "\n",
    "    for a in range(len(input_ids) - len(answer_encoding_fast['input_ids'])):\n",
    "        if input_ids[a + 1: a + 1 + len(answer_encoding_fast['input_ids']) - 2] == answer_encoding_fast['input_ids'][1:-1]:\n",
    "            ret_start = a + 1\n",
    "            ret_end = ret_start + len(answer_encoding_fast['input_ids']) - 2\n",
    "            break\n",
    "    return ret_start, ret_end\n",
    "\n",
    "def update_positions(encodings, answers):\n",
    "    start_positions, end_positions = [], []\n",
    "    ctr = 0\n",
    "    for h in range(len(encodings['input_ids'])):\n",
    "        s, e = ret_Answer_start_and_end(encodings, answers, h)\n",
    "        start_positions.append(s)\n",
    "        end_positions.append(e)\n",
    "        if s == 0:\n",
    "            ctr += 1\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    print(ctr)\n",
    "\n",
    "update_positions(train_encodings_fast, train_answers)\n",
    "update_positions(valid_encodings_fast, valid_answers)\n",
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {key: torch.tensor(val[i]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = InputDataset(train_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            nn.Linear(768 * 2, 768 * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(768 * 2, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = torch.cat((model_output.hidden_states[-1], model_output.hidden_states[-3]), dim=-1)\n",
    "        logits = self.linear_relu_stack(hidden_states)\n",
    "        \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
    "\n",
    "model = QAModel()\n",
    "\n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    return (start_loss + end_loss) / 2\n",
    "\n",
    "def focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma=1):\n",
    "    smax = nn.Softmax(dim=1)\n",
    "    inv_probs_start = 1 - smax(start_logits)\n",
    "    inv_probs_end = 1 - smax(end_logits)\n",
    "\n",
    "    lsmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start, log_probs_end = lsmax(start_logits), lsmax(end_logits)\n",
    "    \n",
    "    nll = nn.NLLLoss()\n",
    "    fl_start = nll(inv_probs_start.pow(gamma) * log_probs_start, start_positions)\n",
    "    fl_end = nll(inv_probs_end.pow(gamma) * log_probs_end, end_positions)\n",
    "    \n",
    "    return (fl_start + fl_end) / 2\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "EPOCHS = 3\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=len(train_dataset) * EPOCHS)\n",
    "\n",
    "total_acc, total_loss, f1_scores = [], [], []\n",
    "\n",
    "def train_epoch(model, dataloader, epoch):\n",
    "    model.train()\n",
    "    losses, acc = [], []\n",
    "    batch_tracker = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc='Running Epoch'):\n",
    "        optim.zero_grad()\n",
    "        input_ids, attention_mask, token_type_ids = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['token_type_ids'].to(device)\n",
    "        start_positions, end_positions = batch['start_positions'].to(device), batch['end_positions'].to(device)\n",
    "\n",
    "        out_start, out_end = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = focal_loss_fn(out_start, out_end, start_positions, end_positions)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        start_pred, end_pred = torch.argmax(out_start, dim=1), torch.argmax(out_end, dim=1)\n",
    "        acc.extend([(start_pred == start_positions).float().mean().item(), (end_pred == end_positions).float().mean().item()])\n",
    "        \n",
    "        batch_tracker += 1\n",
    "        if batch_tracker == 250 and epoch == 1:\n",
    "            total_acc.append(sum(acc))\n",
    "            total_loss.append(sum(losses) / len(losses))\n",
    "            batch_tracker = 0\n",
    "    scheduler.step()\n",
    "    return sum(acc) / len(acc), sum(losses) / len(losses)\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    acc, answer_list, f1_true, f1_pred = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Running Evaluation'):\n",
    "            input_ids, attention_mask, token_type_ids = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['token_type_ids'].to(device)\n",
    "            start_true, end_true = batch['start_positions'].to(device), batch['end_positions'].to(device)\n",
    "\n",
    "            out_start, out_end = model(input_ids, attention_mask, token_type_ids)\n",
    "            start_pred, end_pred = torch.argmax(out_start, dim=1), torch.argmax(out_end, dim=1)\n",
    "            f1_true.extend(start_true.cpu().numpy())\n",
    "            f1_pred.extend(start_pred.cpu().numpy())\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_pred:end_pred]))\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "            answer_list.append([answer, tanswer])\n",
    "            acc.extend([(start_pred == start_true).item(), (end_pred == end_true).item()])\n",
    "    \n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(f1_true, f1_pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return answer_list\n",
    "\n",
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "model.to(device)\n",
    "wer_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, epoch + 1)\n",
    "    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n",
    "    answer_list = eval_model(model, valid_data_loader)\n",
    "    \n",
    "    pred_answers, true_answers = [], []\n",
    "    for pred, true in answer_list:\n",
    "        pred_answers.append(pred if pred else \"$\")\n",
    "        true_answers.append(true if true else \"$\")\n",
    "    \n",
    "    wer_score = wer.compute(predictions=pred_answers, references=true_answers)\n",
    "    print(\"epoch\", epoch, \":\", wer_score)\n",
    "    wer_list.append(wer_score)\n",
    "\n",
    "with open(\"base_model_wer.txt\", 'w') as f:\n",
    "    for s in wer_list:\n",
    "        f.write(f\"{s}\\n\")\n",
    "\n",
    "print(\"WER List:\", wer_list)\n",
    "print(\"F1 Scores:\", f1_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
